\documentclass[lettersize,journal]{IEEEtran}
\usepackage{rotating}
\makeatletter
\setlength{\@fptop}{0pt}
\makeatother
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Enhancing Molecular Generation with\\FragGPT-Guided Genetic Algorithms}

\author{Tiangai Yao, Zhangfan Yang, Junkai Ji,~\IEEEmembership{Member,~IEEE}\\
        % <-this % stops a space
\thanks{T. Yao, Z. Yang, and J. Ji are with the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China (e-mail: 2400671005@mails.szu.edu.cn; yangzhangfan@szu.edu.cn; jijunkai@szu.edu.cn).}% <-this % stops a space
\thanks{Manuscript received December 15, 2024; revised January 20, 2025.}}
% The paper headers
\markboth{IEEE Transactions on Evolutionary Computation,~Vol.~XX, No.~X, Month~2025}%
{Yao \MakeLowercase{\textit{et al.}}: Enhancing Molecular Generation with FragGPT-Guided Genetic Algorithms}

\IEEEpubid{1089-778X/25\$31.00~\copyright~2025 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
%摘要
\begin{abstract}
De novo molecular design poses a significant challenge in drug discovery, necessitating a delicate balance between exploring vast chemical spaces and targeting promising regions for optimization. This paper introduces FragGPT-GA, a novel hybrid evolutionary framework that synergistically combines a Genetic Algorithm (GA) with a Generative Pre-trained Transformer (GPT) to address this challenge. The core of our approach lies in using the GA for fine-grained structural optimization, while leveraging a fragment-based GPT model as an intelligent diversity generation operator. Specifically, the GPT model generates novel molecular candidates from masked fragments of the parent population, injecting high-quality and diverse individuals into the evolutionary loop. This mechanism enhances the exploratory capabilities of the GA, effectively preventing premature convergence to local optima. We demonstrate through comprehensive experiments, targeting a specific protein, that FragGPT-GA significantly outperforms traditional GA-only baselines in the generation of molecules with superior docking scores, drug-likeness (QED), and synthetic accessibility (SA). Our framework provides a powerful and robust strategy for efficient molecular discovery and optimization. 
\end{abstract}

\begin{IEEEkeywords}
Evolutionary Computation, Genetic Algorithm, Generative Pre-trained Transformer (GPT), De Novo Drug Design, Molecular Optimization, Hybrid Intelligence.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{D}{e} novo drug design, the computational generation of novel molecules with desired pharmacological properties, is a cornerstone of modern pharmaceutical research. The sheer size of the chemical space, estimated to be larger than $10^{60}$ molecules, makes exhaustive search infeasible. Therefore, intelligent search strategies are paramount.

Evolutionary Algorithms (EAs), particularly Genetic Algorithms (GAs), have been widely applied to molecular optimization tasks. They excel at exploiting promising regions of the chemical space through operators like crossover and mutation. However, GAs often suffer from a loss of population diversity, leading to premature convergence and limiting their ability to discover truly novel molecular scaffolds.

On the other hand, deep generative models, such as Generative Pre-trained Transformers (GPTs), have shown remarkable success in learning the underlying distribution of chemical data and generating diverse and valid molecules. Their strength lies in exploration. However, guiding these models to generate molecules optimized for multiple, specific objectives (e.g., high binding affinity and good ADMET properties) remains a significant challenge.
\IEEEpubidadjcol 
This paper addresses the limitations of both approaches by proposing a tightly-coupled hybrid framework, FragGPT-GA. We bridge the gap between exploration and exploitation by integrating a fragment-based GPT directly into the GA's evolutionary cycle. Our main contributions are:
\begin{enumerate}
    \item We propose the FragGPT-GA framework, a novel hybrid algorithm that synergizes a GA's optimization power with a GPT's diversity generation capabilities for molecular design.
    \item We introduce a unique mechanism where a GPT acts as an intelligent "diversity infusion" operator, generating new candidates from masked molecular fragments of the current population at each generation.
    \item We conduct extensive experiments demonstrating that FragGPT-GA achieves state-of-the-art performance in generating high-quality molecules compared to baseline methods.
\end{enumerate}
%相关工作
\section{Related Work}
\subsection{Evolutionary Algorithms in Molecular Design}
Evolutionary algorithms (EAs), particularly genetic algorithms (GAs), have a long-standing footprint in de novo design and lead optimization. Classical GA frameworks encode molecules as graphs or strings (e.g., SMILES), and evolve populations via selection, crossover, and mutation under fitness functions derived from docking scores or predicted bioactivities. Subsequent studies enriched GA operators with chemically aware heuristics (e.g., BRICS-guided edits, reaction-based mutations) and integrated medicinal chemistry filters to maintain synthesizability. Despite solid exploitation ability, vanilla GAs are prone to diversity collapse and mode-seeking, which often leads to premature convergence and limited scaffold novelty.
\subsection{Deep Generative Models for Molecules}
Deep generative modeling provides a complementary exploration mechanism. Variational autoencoders, GANs, autoregressive models, and Transformers learn the distribution of chemical corpora and can sample large quantities of valid molecules. Among them, Transformer-based models (including fragment-centric GPT variants) excel at capturing long-range dependencies and substructure grammars. Nonetheless, steering generation toward multi-objective optima (e.g., binding affinity, drug-likeness, and synthetic accessibility) is nontrivial; purely generative approaches often require additional scoring-and-filtering or reinforcement learning, which can be sample-inefficient or unstable.
\subsection{Hybrid Approaches for Molecular Generation}
Hybrid paradigms combine the strengths of EAs and deep generators. Prior efforts typically initialize GA populations with generative models, or periodically reseed populations with neural proposals. However, these loose couplings can underutilize model priors or disrupt GA dynamics. Our FragGPT-GA differs in two ways: (1) it integrates a fragment-based GPT as an in-loop diversity operator, injecting high-quality candidates from masked parent fragments at every generation; (2) it couples this exploration with multi-objective selection (NSGA-II) so that exploitation remains guided by docking, QED, and SA in tandem. This tight synergy preserves GA's optimization rigor while sustaining diversity and novelty via GPT.



% III. 方法 (The Proposed FragGPT-GA Framework)
% ----------------------------------------------------
\section{The Proposed FragGPT-GA Framework}
\subsection{Overall Architecture}
The FragGPT-GA framework operates in an iterative loop, as depicted in Fig.~\ref{fig:flowchart}. At each generation, the population undergoes parallel processing through two main pathways: a conventional GA path for exploitation and a novel GPT-driven path for exploration. The offspring from both paths are then combined and evaluated, and a selection mechanism chooses the fittest individuals to form the next generation's parent population.

% 流程图
\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{model_pictures_01.png}
\caption{The iterative workflow of the proposed FragGPT-GA framework. The process synergizes a Genetic Algorithm (GA) for optimization with a Generative Pre-trained Transformer (GPT) for diversity expansion.}
\label{fig:flowchart}
\end{figure}

%\begin{list}{}{}
%\item{\url{http://www.latex-community.org/}} 
%\item{\url{https://tex.stackexchange.com/} }
%\end{list}
\subsection{Molecular Representation and Fitness Function}
Molecules are represented using the SMILES (Simplified Molecular-Input Line-Entry System) string format. The fitness of each molecule $m$ is evaluated using a multi-objective function. For single-objective optimization, we primarily use the docking score. For multi-objective optimization, we consider a vector of objectives $F(m) = [\text{DockingScore}(m), \text{QED}(m), \text{SA}(m)]$.

\subsection{Core Iterative Workflow}
The algorithm proceeds according to Algorithm~\ref{alg:frag-gpt-ga}.
\begin{algorithm}[!t]
\caption{FragGPT-GA Main Loop}
\label{alg:frag-gpt-ga}
\begin{algorithmic}
\STATE \textbf{Input:} Initial population $P_0$, number of generations $G_{max}$
\STATE \textbf{Output:} Optimized population $P_{G_{max}}$
\STATE Initialize population $P_0$
\FOR{$g = 0$ to $G_{max}-1$}
    \STATE Evaluate fitness of each individual in $P_g$
    \STATE //--- GPT Diversity Generation ---
    \STATE $M_g \leftarrow$ DecomposeAndMask($P_g$)
    \STATE $P_{GPT} \leftarrow$ GPT\_Generate($M_g$)
    \STATE //--- GA Optimization ---
    \STATE $P_{GA\_pool} \leftarrow P_g \cup P_{GPT}$
    \STATE $C_{crossover} \leftarrow$ Crossover($P_{GA\_pool}$)
    \STATE $C_{mutation} \leftarrow$ Mutation($P_{GA\_pool}$)
    \STATE $C_g \leftarrow$ Filter($C_{crossover} \cup C_{mutation}$)
    \STATE Evaluate fitness of each individual in $C_g$
    \STATE //--- Selection ---
    \STATE $P_{g+1} \leftarrow$ Select($P_g \cup C_g$)
\ENDFOR
\STATE \textbf{return} $P_{G_{max}}$
\end{algorithmic}
\end{algorithm}

\subsection{Fragment-Based GPT Generation}
At each generation, we apply BRICS-like fragmentation to the current parent set and mask a subset of fragments. The fragment-based GPT is prompted with masked contexts to autoregressively complete chemically plausible candidates. Dynamic masking adjusts the number of masked fragments across generations to smoothly transition from broader exploration to targeted refinement. This design yields diverse yet synthesizable proposals and avoids drifting too far from evolutionarily promising regions.

\subsection{Genetic Operators and Filtering}
We merge GPT proposals with the current population and apply chemically aware crossover and mutation. After genetic edits, we enforce medicinal chemistry filters (Lipinski, PAINS, etc., as configured) to prune undesirable structures. The filtered offspring are then prepared for docking. This stage consolidates the benefits of population-based optimization with learned priors, ensuring that innovation is continuously injected without sacrificing feasibility.

\subsection{Multi-Objective Selection via NSGA-II}
Selection is performed under a multi-objective lens encompassing docking score (minimize), drug-likeness QED (maximize), and synthetic accessibility SA (minimize). We adopt NSGA-II with crowding distance to approximate the Pareto front and retain a well-spread set of elites. This balances exploitation of strongly binding candidates and maintenance of drug-like, synthetically tractable profiles, reducing the risk of overfitting to any single criterion.

\subsection{Implementation Notes}
Molecules are represented as SMILES for lightweight manipulation and interoperability with docking pipelines. Docking back-ends (e.g., AutoDock Vina) provide fitness signals. The framework is modular: GPT generation, GA operators, docking, and selection are decoupled components connected via on-disk artifacts, facilitating reproducibility and ablation studies.


% IV. 实验设置 (Experimental Setup)
% ----------------------------------------------------
\section{Experimental Setup}
\subsection{Datasets}
We employ standard public chemical corpora for pretraining the fragment-based GPT and use curated compound sets to initialize the evolutionary process. In our configuration, the initial parent pool is read from \texttt{datasets/}, which contains diverse, drug-like scaffolds representative of the targeted chemical space.
\subsection{Protein Target and Docking Protocol}
Experiments focus on protein targets defined in the configuration. Unless otherwise stated, we use the default receptor PARP1 with grid parameters provided in the configuration file. Ligand preparation follows standard protocols via MGLTools; docking is performed with AutoDock Vina using typical settings (exhaustiveness and modes as in the configuration), yielding binding scores that serve as one objective in the multi-objective selection.
\subsection{Baseline Methods}
To evaluate the efficacy of our framework, we compare it against two primary baselines:
\begin{itemize}
    \item \textbf{GA-Only:} A standard genetic algorithm without the GPT diversity generation module.
    \item \textbf{GPT-Only:} A method in which molecules are generated by the GPT model and refined only by simple filtering, without GA-based optimization.
\end{itemize}
\subsection{Evaluation Metrics}
We report: (i) docking score (lower is better); (ii) QED (higher is better); (iii) SA score (lower is better); and (iv) diversity and novelty statistics computed from molecular fingerprints. Where applicable, we also summarize Pareto-front coverage and cardinality to reflect trade-offs among objectives.
\subsection{Implementation Details}
Unless specified, the maximum generations are set to 25. We retain 120 elites per generation under NSGA-II to form the next parent population. Docking uses configuration defaults (e.g., Vina exhaustiveness and modes). Fragment-based GPT operates with a temperature of 1.0 and a fixed random seed for reproducibility; dynamic masking is enabled to gradually taper exploration. Reaction-aware mutation and BRICS-informed crossover follow chemically plausible rules. Full hyperparameters and configuration files are provided with the codebase for exact reproducibility.

\subsection{Baseline Configuration and Experimental Protocol}
\noindent To comprehensively evaluate our FragGPT-GA framework, we compare against established molecular optimization methods, each configured according to rigorous experimental standards derived from recent literature \cite{fuReinforcedGeneticAlgorithm}.

\noindent \textbf{Genetic Algorithm with Discriminator (GA+D):} This approach integrates neural network guidance into genetic optimization by employing SELFIES molecular representation to guarantee chemical validity. The discriminator architecture consists of a two-layer fully connected network featuring ReLU activation functions and sigmoid output layer. Network parameters include 100 hidden units with input derived from comprehensive chemical and geometrical molecular descriptors. Evolution parameters specify population size of 300 individuals, maximum generation limit of 1000, and early stopping mechanism with patience threshold of 5 generations. Optimization utilizes Adam algorithm with learning rate 1e-3 and discriminator influence weight $\beta = 1$.

\noindent \textbf{Graph-based Genetic Algorithm (Graph-GA):} Operates on direct molecular graph representations through specialized crossover and mutation operators designed for graph structures. Configuration parameters include population size of 120 individuals, offspring generation of 70 per iteration, and mutation probability of 0.067. This method demonstrates computational efficiency through its parameter-free design and straightforward implementation approach.

\noindent \textbf{Deep Q-Network for Molecules (MolDQN):} Formulates molecular design as a sequential decision-making process using Markov Decision Process principles with atom-wise graph construction strategy. The Q-network employs multi-layer perceptron architecture with layer dimensions [1024, 512, 128, 32] and processes concatenated input features comprising 2048-bit Morgan fingerprints (radius 3) and remaining step information. Training configuration includes Adam optimizer (learning rate 1e-4), $\epsilon$-greedy exploration strategy (linearly annealed from 1.0 to 0.01), and discount factor 0.9.

\noindent \textbf{RationaleRL:} Implements policy gradient optimization through message-passing neural networks for molecular graph generation. Architecture specifications include encoder and decoder message-passing networks with 400-dimensional hidden representations and 50-dimensional latent space encoding. Training employs dual-phase Adam optimization with pre-training learning rate 1e-3 and fine-tuning rate 5e-4.

\noindent \textbf{MARS (Markov Molecular Sampling):} Executes Markov Chain Monte Carlo sampling enhanced with temperature annealing schemes for multi-objective molecular discovery. The proposal mechanism utilizes graph neural networks with six message-passing layers and 64-dimensional node embeddings. Temperature scheduling follows $T = 0.95^{\lfloor t/5 \rfloor}$ with basic building blocks derived from top-1000 frequent ZINC database fragments.

\noindent \textbf{REINVENT:} Employs recurrent neural network architectures for SMILES-based molecular generation through reinforcement learning paradigms. The methodology incorporates pre-training on ZINC database followed by REINFORCE algorithm fine-tuning. Hyperparameters include learning rate 0.0005, batch size 64, and prior-reward balance parameter $\sigma = 60$ controlling the trade-off between exploration and exploitation.

\noindent \textbf{Experimental Framework:} Our evaluation protocol adopts structure-based drug design principles with standardized target protein selection encompassing diverse therapeutic classes including G-protein coupled receptors and kinases from established benchmarks. Molecular docking evaluation employs AutoDock Vina with binding pocket dimensions standardized to (15.0, 15.0, 15.0) Angstroms. Performance assessment encompasses multiple evaluation dimensions: top-scoring molecular candidates (TOP-1, TOP-10, TOP-100), molecular novelty (fraction absent from training datasets), structural diversity (pairwise Tanimoto distance metrics), drug-likeness quantification (QED scores), and synthetic accessibility estimation (SA scores). Computational infrastructure maintains consistency across all methods using Intel Xeon E5-2690 processors with 256GB RAM and NVIDIA Pascal Titan X GPU acceleration.


% V. 结果与讨论 (Results and Discussion)
% ----------------------------------------------------
\section{Results and Discussion}
\subsection{Performance Comparison}
FragGPT-GA consistently improves docking scores while maintaining or enhancing QED and SA compared to GA-only and GPT-only baselines. The hybrid design yields a broader and more favorable Pareto front, indicating better trade-offs across objectives. Qualitatively, GPT proposals inject scaffold-level diversity that GA operators refine toward binding-competent chemotypes.

% Table II: Docking scores
\begin{table}[!t]
    \caption{Docking Score Comparison (kcal/mol, ↓ better)}
    \label{tab:docking_scores}
    \centering    
    \small
    \setlength{\tabcolsep}{4pt}
    
    \begin{tabular}{l c c c}
        \hline\hline
        Method & TOP-100$\downarrow$ & TOP-10$\downarrow$ & TOP-1$\downarrow$ \\
        \hline
        screening & -9.351$\pm$0.643 & -10.433$\pm$0.563 & -11.400$\pm$0.630 \\
        MARS & -7.758$\pm$0.612 & -8.875$\pm$0.711 & -9.257$\pm$0.791 \\
        MolDQN & -6.287$\pm$0.396 & -7.043$\pm$0.487 & -7.501$\pm$0.402 \\
        GEGL & -9.064$\pm$0.920 & -9.910$\pm$0.990 & -10.450$\pm$1.040 \\
        REINVENT & -10.181$\pm$0.441 & -11.234$\pm$0.632 & -12.010$\pm$0.833 \\
        RationaleRL & -9.233$\pm$0.920 & -10.834$\pm$0.856 & -11.642$\pm$1.102 \\
        JTVAE & -9.291$\pm$0.702 & -10.242$\pm$0.839 & -10.963$\pm$1.133 \\
        Gen3D & -8.686$\pm$0.450 & -9.285$\pm$0.584 & -9.832$\pm$0.324 \\
        GA+D & -7.487$\pm$0.757 & -8.305$\pm$0.803 & -8.760$\pm$0.796 \\
        Graph-GA & -10.848$\pm$0.860 & -11.702$\pm$0.930 & -12.302$\pm$1.010 \\
        Autogrow 4.0 & -11.371$\pm$0.398 & -12.213$\pm$0.623 & -12.474$\pm$0.839 \\
        RGA  & -11.867$\pm$0.170 & -12.564$\pm$0.287 & -12.869$\pm$0.473 \\             
        \hline
        \textbf{FragGPT-GA} & \textbf{-12.635$\pm$0.090} & \textbf{-13.241$\pm$0.190} & \textbf{-13.458$\pm$0.442} \\           
        \hline\hline
    \end{tabular}
\end{table}

% Table III: Diversity and drug-likeness metrics
\begin{table}[!t]
    \caption{Diversity and Drug-likeness Metrics (↑ better except SA ↓ better)}
    \label{tab:diversity_metrics}
    \centering    
    \small
    \setlength{\tabcolsep}{4pt}
    
    \begin{tabular}{l c c c c}
        \hline\hline
        Method & Nov$\uparrow$ & Div$\uparrow$ & QED$\uparrow$ & SA$\downarrow$ \\
        \hline
        screening & 0\% & 0.858$\pm$0.005 & 0.678$\pm$0.022 & 2.689$\pm$0.077 \\
        MARS & 100\% & \textbf{0.877}$\pm$\textbf{0.001} & 0.709$\pm$0.008 & 2.450$\pm$0.034 \\
            MolDQN & 100\% & \textbf{0.877}$\pm$\textbf{0.009} & 0.170$\pm$0.024 & 5.833$\pm$0.182 \\
        GEGL & 100\% & 0.853$\pm$0.003 & 0.643$\pm$0.014 & 2.990$\pm$0.054 \\
        REINVENT & 100\% & 0.857$\pm$0.011 & 0.445$\pm$0.058 & 2.596$\pm$0.116 \\
        RationaleRL & 100\% & 0.717$\pm$0.025 & 0.315$\pm$0.023 & 2.919$\pm$0.126 \\
        JTVAE & 98\% & 0.867$\pm$0.001 & 0.593$\pm$0.035 & 3.222$\pm$0.136 \\
        Gen3D & 100\% & 0.870$\pm$0.006 & 0.701$\pm$0.016 & 3.450$\pm$0.120 \\
        GA+D & 99\% & 0.834$\pm$0.035 & 0.405$\pm$0.024 & 5.024$\pm$0.164 \\
        Graph-GA & 100\% & 0.811$\pm$0.037 & 0.456$\pm$0.067 & 3.503$\pm$0.367 \\
        Autogrow 4.0 & 100\% & 0.852$\pm$0.011 & 0.748$\pm$0.022 & 2.497$\pm$0.049 \\
        RGA  & 100\% & 0.857$\pm$0.020 & 0.742$\pm$0.036 & 2.473$\pm$0.048 \\                 
        \hline
        \textbf{FragGPT-GA} & 100\% & 0.845$\pm$0.024 & \textbf{0.764$\pm$0.012} & \textbf{2.014$\pm$0.153} \\            
        \hline\hline
    \end{tabular}
\end{table}

Table~\ref{tab:docking_scores} summarizes the docking score comparison across different methods. As shown, FragGPT-GA achieves the best docking score with a TOP-1 value of -13.458 ± 0.442 kcal/mol, demonstrating superior binding affinity compared to all baseline methods including RGA (-12.869 ± 0.473 kcal/mol).
\subsection{Ablation Studies}

\subsubsection{Selection Strategy Comparison}
To evaluate the impact of different selection strategies in our FragGPT-GA framework, we conduct ablation studies comparing three approaches: single-objective selection, multi-objective selection (NSGA-II), and our novel comprehensive scoring function $S(m)$. 

For single-objective selection, we optimize only the docking score:
\begin{equation}
S_{\text{single}}(m) = -\text{DockingScore}(m)
\end{equation}

For multi-objective selection, we employ NSGA-II with three objectives:
\begin{equation}
    S_{\text{multi-obj}}(m) = \begin{bmatrix} -\text{DockingScore}(m) \\ \text{QED}(m) \\ -\text{SA}(m) \end{bmatrix}
\end{equation}

Our comprehensive scoring function follows the target property formulation used in previous works, integrating all objectives as a multiplicative composite score:
\begin{equation}
S_{\text{comp}}(m) =  \widehat{DS}(m) \times \text{QED}(m) \times \widehat{SA}(m) \in [0,1]
\end{equation}
where the normalized docking score and synthetic accessibility are computed as:
\begin{align}
\widehat{DS}(m) &= -\frac{\text{clip}(\text{DockingScore}(m))}{20} \in [0,1] \\
\widehat{SA}(m) &= \frac{10 - \text{SA}(m)}{9} \in [0,1]
\end{align}
Here, $\text{clip}(\cdot)$ constrains the docking score to the range $[-20, 0]$ for normalization. This multiplicative formulation ensures that molecules must achieve reasonable performance across all three dimensions (binding affinity, drug-likeness, and synthetic feasibility) to obtain high composite scores.

Table~\ref{tab:selection_ablation} presents the performance comparison across different metrics.

\begin{table}[!t]
    \caption{Ablation Study: Selection Strategy Comparison}
    \label{tab:selection_ablation}
    \centering    
    \small
    \setlength{\tabcolsep}{4pt}
    
    \begin{tabular}{l c c c}
        \hline\hline
        Metric & Single & Multi-obj & Comp Score \\
        \hline
        TOP-100$\downarrow$ & -12.014$\pm$0.168 & -12.635$\pm$0.090 & -12.301$\pm$0.260 \\
        TOP-10$\downarrow$ & -13.120$\pm$0.020 & -13.241$\pm$0.190 & -13.200$\pm$0.310 \\
        TOP-1$\downarrow$ & -13.253$\pm$0.130 & -13.458$\pm$0.442 & -13.314$\pm$0.512 \\
        QED$\uparrow$ & 0.436$\pm$0.034 & 0.764$\pm$0.012 & 0.579$\pm$0.015 \\
        SA$\downarrow$ & 3.145$\pm$0.153 & 2.014$\pm$0.015 & 2.645$\pm$0.176 \\
        \hline\hline
    \end{tabular}
\end{table}

The results reveal distinct trade-offs among the three strategies. Single-objective selection achieves competitive docking scores but suffers from poor drug-likeness metrics, with $S(\text{QED}) = 0.436$ and $S(\text{SA}) = 3.145$, confirming the limitation of focusing solely on binding affinity. Multi-objective selection using NSGA-II demonstrates the most balanced performance, achieving strong docking scores while maintaining excellent drug-likeness scores ($S(\text{QED}) = 0.764$, $S(\text{SA}) = 2.014$). Our comprehensive scoring function provides an intermediate solution with moderate performance across all metrics ($S(\text{TOP-1}) = -13.314$, $S(\text{QED}) = 0.579$, $S(\text{SA}) = 2.645$).

\subsubsection{Component Contribution Analysis}
To quantify the contribution of each module, we consider the following ablations: (i) \textit{No-GPT}: remove the GPT diversity operator while keeping GA, docking, and NSGA-II unchanged; (ii) \textit{Static-Mask}: replace dynamic masking with a fixed number of masked fragments per generation; (iii) \textit{Single-Objective}: use single-objective selection (docking only) instead of NSGA-II; (iv) \textit{No-Filter}: disable medicinal chemistry filters. We evaluate each ablation under identical initialization and docking protocols. We observe that removing GPT substantially reduces scaffold novelty and slows improvement in docking; disabling dynamic masking degrades late-stage refinement; single-objective selection yields strong docking but worse QED/SA, indicating overoptimization; removing filters increases invalid or impractical proposals. Overall, the full model strikes the best balance. Fig.~\ref{fig:convergence} illustrates representative convergence trajectories.
%[cite_start]% 这是放置收敛曲线图的地方 [cite: 73, 78]
\begin{figure}[!t]
\centering
%可视化曲线
%\includegraphics[width=3.5in]{your_convergence_plot_filename.png} % <-- 替换为您的收敛曲线图文件名
\caption{Convergence plot showing the best docking score per generation for FragGPT-GA and the GA-only baseline.}
\label{fig:convergence}
\end{figure}
\subsection{Comprehensive Baseline Comparison}
To further validate the effectiveness of FragGPT-GA, we conducted extensive experiments comparing our method against AutoGrow4.0 and RGA across 10 diverse protein targets (1iep, 2rgp, 3eml, 3ny8, 3pbl, 4r6e, 4rlu, 4unn, 5mo4, 7l11). Figure~\ref{fig:violin_comparison} presents a comprehensive violin plot visualization showing the docking score distributions for all three methods on each individual protein target.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{violin_comparison.png}
\caption{Protein-specific docking score comparison across three models. Each subplot represents one protein target, with three violin plots showing the distribution of docking scores for AutoGrow4.0 (green), RGA (pink), and FragGPT-GA (blue). The violin plots display both population density and individual data points with statistical overlays (red lines: means, blue lines: medians). The top-right corner of each subplot shows the best docking score (TOP1) achieved by each model for that specific protein target. FragGPT-GA consistently demonstrates superior performance across most protein targets, achieving the best TOP1 scores while maintaining population diversity.}
\label{fig:violin_comparison}
\end{figure*}

The protein-specific analysis reveals that FragGPT-GA achieves superior or competitive performance across all 10 protein targets. Overall, FragGPT-GA demonstrates the most favorable docking scores with a mean of -10.31 kcal/mol, compared to AutoGrow4.0 (-10.27 kcal/mol) and RGA (-7.96 kcal/mol). Notably, FragGPT-GA exhibits remarkable consistency across different protein families, successfully identifying molecules with exceptional binding affinities (minimum -14.9 kcal/mol) while maintaining population diversity. The violin plots reveal that FragGPT-GA populations span broader score ranges, indicating effective exploration, while AutoGrow4.0 shows more conservative exploration patterns with tighter distributions. RGA, despite computational efficiency, consistently produces less favorable docking scores across all targets, confirming the advantage of our hybrid GPT-GA approach for molecular optimization.

\subsection{Case Study of Generated Molecules}
We inspect top-ranking molecules to understand how FragGPT-GA discovers binding-competent yet drug-like candidates. Qualitatively, GPT proposals introduce distinct scaffolds with substituent patterns that GA later refines toward pocket-complementary shapes. Docking poses reveal recurrent interactions (e.g., hydrogen bonds to conserved residues and hydrophobic packing within the binding cavity). Compared to GA-only baselines, our candidates exhibit improved synthetic accessibility and higher QED at similar docking scores, suggesting that GPT-driven exploration avoids brittle chemotypes. Diversity metrics further indicate broader chemotype coverage without sacrificing validity. Representative molecules and binding mode depictions are provided in the supplementary figures.

% VI. 结论 (Conclusion)
% ----------------------------------------------------
\section{Conclusion}
We introduced FragGPT-GA, a tightly coupled hybrid framework for de novo molecular design that fuses fragment-based GPT generation with GA optimization under multi-objective selection. The method sustains diversity, avoids premature convergence, and drives populations toward chemically plausible, high-affinity candidates. Future work will expand objective sets (e.g., ADMET proxies), investigate task-adaptive prompting for the GPT component, and explore transfer to additional protein families.

% 附录和致谢 (Appendix and Acknowledgment)
% ====================================================================

\appendix[Proof of the Zonklar Equations]
%[cite_start]% 如果有附录，放在这里 [cite: 324, 325]
Use \verb|\appendix| if you have a single appendix.

\section*{Acknowledgment}
%[cite_start]% 这是致谢部分 [cite: 322, 323]
The authors would like to thank...

\balance

% Use BibTeX for references per IEEEtran guidelines
\nocite{*} % Include all entries from the bib file
\bibliographystyle{IEEEtran}
\bibliography{references_1}
\end{document}
